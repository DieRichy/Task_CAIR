{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5974f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/Users/frankdzzz/models/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    force_download=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e7e086fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f078a81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model (LLM) is a type of artificial intelligence that can generate human-like text based on the patterns and statistical probabilities learned from vast amounts of textual data. These models are trained on enormous datasets, such as books, articles, websites, and other written sources, allowing them to understand context, grammar, and semantics deeply.\\n\\nKey features of large language models include:\\n\\n1. **Training**: They are typically trained using techniques like deep learning, where neural networks learn to predict the next word in a sequence based on previous words. This process involves understanding the relationships between words and phrases within a given context.\\n\\n2. **Generative Power**: Once trained, these models can take an input prompt and generate coherent text that is often indistinguishable from human-written content. They can answer questions, write stories, compose emails, and perform a wide range of tasks.\\n\\n3. **Context Awareness**: Large language models are designed to understand the context in which they operate, making them capable of providing relevant and meaningful responses even when given partial or incomplete information.\\n\\n4. **Adaptability**: Many LLMs can be fine-tuned for specific domains or applications, enabling them to specialize in certain areas while maintaining their broad capabilities.\\n\\n5. **Ethical Considerations**: The use of LLMs raises ethical concerns related to bias, privacy, and accountability. Ensuring that these models are used responsibly and transparently is crucial.\\n\\nLLMs have significant applications across various fields, including but not limited to natural language processing, content generation, customer service, and more. They continue to evolve with advancements in machine learning and computational power, pushing the boundaries of what AI can achieve in terms of language understanding and generation.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c877c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\n\\n    \\n\\n  \\n\\r\\n\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1ad3af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "62ab1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "input_text = \"<|im_start|>system\\nAnswer only with 'Yes' or 'No'<|im_end|>\\n<|im_start|>user\\nIs Paris the capital of France<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(model.device)  # 确保 attention_mask 定义\n",
    "\n",
    "with torch.inference_mode():\n",
    "    response = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_logits=True,\n",
    "        return_dict_in_generate=True,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a1eecb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateDecoderOnlyOutput(sequences=tensor([[151644,   8948,    198,  16141,   1172,    448,    364,   9454,      6,\n",
       "            476,    364,   2753,      6, 151645,    198, 151644,    872,    198,\n",
       "           3872,  12095,    279,   6722,    315,   9625, 151645,    198, 151644,\n",
       "          77091,    198,   2753]], device='mps:0'), scores=None, logits=(tensor([[ 7.5625, 10.0625, 13.3750,  ...,  2.0469,  2.0469,  2.0469]],\n",
       "       device='mps:0'),), attentions=None, hidden_states=None, past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "efda4ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.5625, 10.0625, 13.3750,  ...,  2.0469,  2.0469,  2.0469]],\n",
       "        device='mps:0'),)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94ada9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a5a8f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = response.logits[0][0].squeeze(0).cpu().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0730fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nAnswer only with 'Yes' or 'No'<|im_end|>\\n<|im_start|>user\\nIs Paris the capital of France<|im_end|>\\n<|im_start|>assistant\\nNo\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check token generated\n",
    "tokens = tokenizer.decode(response.sequences[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "726f5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "token_id = np.argmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "44a764db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer.decode(token_id)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c07b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c888e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 IDs: [2753, 9454, 2308, 97976, 8996]\n",
      "Top 5 Tokens: NoYes No-NoNO\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "logits_tensor = response.logits[0][0].squeeze(0)\n",
    "top5_ids = torch.topk(logits_tensor, 5).indices.tolist()\n",
    "top5_tokens = tokenizer.decode(top5_ids)\n",
    "print(f\"Top 5 IDs: {top5_ids}\")\n",
    "print(f\"Top 5 Tokens: {top5_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43ff17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1c463fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = response.logits[0][0].squeeze(0).cpu().numpy()  # shape -> [vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d3394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.5625  , 10.0625  , 13.375   , ...,  2.046875,  2.046875,\n",
       "        2.046875], shape=(151936,), dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c611b787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(44.5)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[2753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9be76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2753)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d4190069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2753])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = response.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4456b48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20412, device='mps:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93de83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_generated_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83c4b73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.03125   , 7.6875    , 5.28125   , ..., 0.73828125, 0.73828125,\n",
       "       0.73828125], shape=(151936,), dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_logits= response.logits[0].squeeze(0).cpu().numpy()\n",
    "first_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3007679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(20412)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_generated_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd7e48ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'是'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([20412])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c19e00f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a short introduction to large language models.强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸强奸 ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculationIENTATION hydration hydrationresponsesassertCountassertCountIENTATIONIENTATIONIENTATION stringByAppendingString hydration hydrationIHIH，《 ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation ejaculation\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 或 \"auto\"\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 创建 pipeline\n",
    "text_gen = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 测试 prompt\n",
    "prompt = \"Give me a short introduction to large language models.\"\n",
    "\n",
    "# 生成文本\n",
    "output = text_gen(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064476a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "188796.20s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/evaluator/scoring.py\n",
    "# This file is to implement scoring functions for pred-vs-gt evaluations\n",
    "# src/scoring.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_rejection_metrics(df, metric_name,):\n",
    "    \"\"\"\n",
    "    计算拒识准确率。\n",
    "    metric_name: 'logprob' 或 'logtoku'\n",
    "    \"\"\"\n",
    "    # Rank from most certain to least certain\n",
    "    # logprob and logtoku: the greater number means the more confident llm is about its prediction closer to 0\n",
    "    \n",
    "    df_sorted = df.sort_values(by=metric_name, ascending=False).reset_index(drop=True)  # from smallest to largest   \n",
    "    \n",
    "    total_samples = len(df_sorted)\n",
    "    results = []\n",
    "\n",
    "    # 计算不同百分位下的准确率 (从 0% （no rejection) 拒绝到 95%拒绝)\n",
    "    # 这里的 i 代表“保留前 i% 的样本”\n",
    "    for percentile in range(0, 100, 5):\n",
    "        keep_ratio = (100 - percentile) / 100\n",
    "        num_keep = max(1, int(total_samples * keep_ratio))\n",
    "\n",
    "        subset = df_sorted.head(num_keep)\n",
    "        accuracy = subset['is_correct'].mean()\n",
    "        \n",
    "        results.append({\n",
    "            \"percentile\": 100 - percentile,  # keep ratio\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ca4d2c47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#src/evaluator/plotting.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m####################     \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#This file is to implement plotting functions for evaluation results\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# src/plotting.py\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/Task_CAIR/.venv/lib/python3.10/site-packages/matplotlib/__init__.py:1299\u001b[0m\n\u001b[1;32m   1295\u001b[0m     rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend_fallback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 1299\u001b[0m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_backend\u001b[39m(\u001b[38;5;241m*\u001b[39m, auto_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;124;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;124;03m    matplotlib.use\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Task_CAIR/.venv/lib/python3.10/site-packages/matplotlib/__init__.py:774\u001b[0m, in \u001b[0;36mRcParams.__setitem__\u001b[0;34m(self, key, val)\u001b[0m\n\u001b[1;32m    772\u001b[0m         cval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate[key](val)\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m--> 774\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set(key, cval)\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']"
     ]
    }
   ],
   "source": [
    "#src/evaluator/plotting.py\n",
    "####################     \n",
    "#This file is to implement plotting functions for evaluation results\n",
    "\n",
    "# src/plotting.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_uncertainty_heatmap(prob_results, toku_results, save_path=\"results/rejection_heatmap.png\"):\n",
    "    \"\"\"\n",
    "    绘制不确定性拒绝结果的热力图\n",
    "    \n",
    "    参数:\n",
    "        prob_results: dict, 包含 'percentile' 和 'accuracy' 键\n",
    "        toku_results: dict, 包含 'percentile' 和 'accuracy' 键\n",
    "        save_path: str, 保存路径\n",
    "    \"\"\"\n",
    "    # 提取数据\n",
    "    percentiles = prob_results['percentile']\n",
    "    prob_accuracy = prob_results['accuracy']\n",
    "    toku_accuracy = toku_results['accuracy']\n",
    "    \n",
    "    # 将数据组织成二维数组 (行: percentile, 列: metrics_accuracy)\n",
    "    data = np.column_stack([prob_accuracy, toku_accuracy])\n",
    "    \n",
    "    # 创建图形\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    \n",
    "    # 绘制热力图\n",
    "    ax = sns.heatmap(data[::-1], \n",
    "                     annot=True,  # 显示数值\n",
    "                     fmt='.3f',   # 数值格式：保留3位小数\n",
    "                     cmap='YlGnBu',  # 颜色方案 (黄绿蓝)\n",
    "                     yticklabels=percentiles[::-1],  # y轴标签 from 100 to 5%\n",
    "                     xticklabels=['acc_w_rejection_prob', 'acc_w_rejection_uncertainty_2'],  # x轴标签\n",
    "                     cbar_kws={'label': ''},  # 颜色条\n",
    "                     vmin=np.min(data), \n",
    "                     vmax=1.0,  # 设置颜色范围\n",
    "                     linewidths=0.5,  # 网格线宽度\n",
    "                     linecolor='white')  # 网格线颜色\n",
    "    \n",
    "    # 设置标签\n",
    "    plt.ylabel('percentile', fontsize=12)\n",
    "    plt.xlabel('')\n",
    "    \n",
    "    # 设置标题\n",
    "    plt.title('Accuracy vs. Rejection Rate (Heatmap)', fontsize=14, pad=20)\n",
    "    \n",
    "    # 调整x轴标签\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    \n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图片\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Heatmap saved to {save_path}\")\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8d5e6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"results/boolq_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "bd3d80fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2000.000000\n",
       "mean       -0.015706\n",
       "std         0.000664\n",
       "min        -0.018054\n",
       "25%        -0.016189\n",
       "50%        -0.015800\n",
       "75%        -0.015260\n",
       "max        -0.013974\n",
       "Name: logtoku, dtype: float64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"logtoku\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "945f0a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.000000e+03\n",
       "mean    -1.692777e-02\n",
       "std      7.455978e-02\n",
       "min     -6.473705e-01\n",
       "25%     -4.169432e-05\n",
       "50%     -2.384186e-07\n",
       "75%      0.000000e+00\n",
       "max      0.000000e+00\n",
       "Name: logprob, dtype: float64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"logprob\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "7c57e59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_correct</th>\n",
       "      <th>logprob</th>\n",
       "      <th>logtoku</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>-5.623774e-01</td>\n",
       "      <td>-0.013974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>-2.191842e-01</td>\n",
       "      <td>-0.013980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>-9.163713e-02</td>\n",
       "      <td>-0.014012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>-2.450537e-01</td>\n",
       "      <td>-0.014025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>-2.888999e-01</td>\n",
       "      <td>-0.014025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>True</td>\n",
       "      <td>-3.576279e-07</td>\n",
       "      <td>-0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>False</td>\n",
       "      <td>-1.192093e-07</td>\n",
       "      <td>-0.017522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>False</td>\n",
       "      <td>-2.384186e-07</td>\n",
       "      <td>-0.017591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.311303e-06</td>\n",
       "      <td>-0.017652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>False</td>\n",
       "      <td>-3.779272e-03</td>\n",
       "      <td>-0.018054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_correct       logprob   logtoku\n",
       "0           True -5.623774e-01 -0.013974\n",
       "1          False -2.191842e-01 -0.013980\n",
       "2          False -9.163713e-02 -0.014012\n",
       "3           True -2.450537e-01 -0.014025\n",
       "4          False -2.888999e-01 -0.014025\n",
       "...          ...           ...       ...\n",
       "1995        True -3.576279e-07 -0.017400\n",
       "1996       False -1.192093e-07 -0.017522\n",
       "1997       False -2.384186e-07 -0.017591\n",
       "1998        True -1.311303e-06 -0.017652\n",
       "1999       False -3.779272e-03 -0.018054\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_2 = df.sort_values(by=\"logtoku\", ascending=False).reset_index(drop=True)\n",
    "df_sorted_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "971c1c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_correct</th>\n",
       "      <th>logprob</th>\n",
       "      <th>logtoku</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.589742</td>\n",
       "      <td>-0.014705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>False</td>\n",
       "      <td>-0.618071</td>\n",
       "      <td>-0.014363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.647370</td>\n",
       "      <td>-0.014953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>False</td>\n",
       "      <td>-0.647370</td>\n",
       "      <td>-0.014053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>True</td>\n",
       "      <td>-0.647370</td>\n",
       "      <td>-0.015106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_correct   logprob   logtoku\n",
       "0           True  0.000000 -0.016420\n",
       "1           True  0.000000 -0.015928\n",
       "2           True  0.000000 -0.015954\n",
       "3           True  0.000000 -0.015755\n",
       "4           True  0.000000 -0.015794\n",
       "...          ...       ...       ...\n",
       "1995        True -0.589742 -0.014705\n",
       "1996       False -0.618071 -0.014363\n",
       "1997        True -0.647370 -0.014953\n",
       "1998       False -0.647370 -0.014053\n",
       "1999        True -0.647370 -0.015106\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_1 = df.sort_values(by='logprob', ascending=False).reset_index(drop=True)  # from smallest to largest   \n",
    "df_sorted_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "3b1a3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    " #3.1 logprob df\n",
    "logprob_df = calculate_rejection_metrics(df, metric_name='logprob',) # the greater logprob means more confident\n",
    "#3.2 logtoku df\n",
    "logtoku_df = calculate_rejection_metrics(df, metric_name='logtoku',) #  the smaller logtoku means more uncertain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "75e6d998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentile</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4</td>\n",
       "      <td>0.726842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9</td>\n",
       "      <td>0.721667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-14</td>\n",
       "      <td>0.715294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-19</td>\n",
       "      <td>0.709375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-24</td>\n",
       "      <td>0.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-29</td>\n",
       "      <td>0.702857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-34</td>\n",
       "      <td>0.696923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-39</td>\n",
       "      <td>0.694167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-44</td>\n",
       "      <td>0.687273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-49</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-54</td>\n",
       "      <td>0.673333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-59</td>\n",
       "      <td>0.663750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-64</td>\n",
       "      <td>0.654286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-69</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-74</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-79</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-84</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-89</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-94</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percentile  accuracy\n",
       "0            1  0.730500\n",
       "1           -4  0.726842\n",
       "2           -9  0.721667\n",
       "3          -14  0.715294\n",
       "4          -19  0.709375\n",
       "5          -24  0.706667\n",
       "6          -29  0.702857\n",
       "7          -34  0.696923\n",
       "8          -39  0.694167\n",
       "9          -44  0.687273\n",
       "10         -49  0.683000\n",
       "11         -54  0.673333\n",
       "12         -59  0.663750\n",
       "13         -64  0.654286\n",
       "14         -69  0.640000\n",
       "15         -74  0.608000\n",
       "16         -79  0.580000\n",
       "17         -84  0.540000\n",
       "18         -89  0.500000\n",
       "19         -94  0.500000"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logtoku_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "801cd43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentile</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.742632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.757222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.758824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.768125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>0.768667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>0.767143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35</td>\n",
       "      <td>0.771538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45</td>\n",
       "      <td>0.771818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55</td>\n",
       "      <td>0.762222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>60</td>\n",
       "      <td>0.746250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>65</td>\n",
       "      <td>0.731429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>0.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>75</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>80</td>\n",
       "      <td>0.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>85</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>90</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>95</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percentile  accuracy\n",
       "0            0  0.730500\n",
       "1            5  0.742632\n",
       "2           10  0.757222\n",
       "3           15  0.758824\n",
       "4           20  0.768125\n",
       "5           25  0.768667\n",
       "6           30  0.767143\n",
       "7           35  0.771538\n",
       "8           40  0.775000\n",
       "9           45  0.771818\n",
       "10          50  0.764000\n",
       "11          55  0.762222\n",
       "12          60  0.746250\n",
       "13          65  0.731429\n",
       "14          70  0.735000\n",
       "15          75  0.736000\n",
       "16          80  0.722500\n",
       "17          85  0.693333\n",
       "18          90  0.685000\n",
       "19          95  0.690000"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2ae83d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = subset['is_correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "847f446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4     True\n",
       "5    False\n",
       "6     True\n",
       "7     True\n",
       "8     True\n",
       "9    False\n",
       "Name: is_correct, dtype: bool"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501aacf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task_CAIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
